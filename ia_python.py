# -*- coding: utf-8 -*-
"""IA python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uIWL0i_DOUMqoxXzFKgfRkTQQz4VtWRX
"""

import numpy as np
import scipy as sc
import matplotlib.pyplot as plt

from sklearn.datasets import make_circles

# crear dataset
n= 500 
p = 2   # n numero de regustros, p numero de caracteristicas 

X,Y = make_circles(n_samples=n, factor=0.4, noise=0.04)

Y=Y[:,np.newaxis] # soluciona error en train Bp
# Y coresponde a un vector binario que indica la pertenencia de cada circulo 
plt.scatter(X[Y[:,0] == 0,0], X[Y[:,0]==0,1], c="skyblue")
plt.scatter(X[Y[:,0]==1,0], X[Y[:,0]==1,1], c="salmon")
plt.axis("equal")
plt.show()

# clase de capa de la red neuronal 
class neural_layer():

  def __init__(self, n_conn, n_neur, act_f):
    self.act_f = act_f

    self.b= np.random.rand(1, n_neur)      *2 - 1
    
    self.W= np.random.rand(n_conn, n_neur) *2 - 1

# funciones de acticvacion
sigm = (lambda x: 1/(1 + np.e **(-x)),
        lambda x: x*(1-x))# derivada de sigmoide

relu = lambda x: np.maximum(0, x)

_x = np.linspace(-5, 5, 1000)
plt.plot(_x, sigm[0](_x))

l0 = neural_layer(p, 4, sigm) # creando capas, 1 a 1 
l1 = neural_layer(4, 8, sigm)

# ..... funcion par acrear red 

def create_nn(topology, act_f):
  nn= []# vctor Neural Networld

  for l, layer in enumerate(topology[:-1]):

    nn.append(neural_layer(topology[l], topology[l+1], act_f))

  return nn

topology = [p, 8, 4, 1] # numero de neuronas por capa 
# la topologia debe tener relacion con la cantidad de datos
neural_net = create_nn(topology, sigm)

l2_cost = (lambda Yp, Yr: np.mean((Yp-Yr)**2),
           lambda Yp, Yr: (Yp-Yr)) #derivada de FdeCoste


def train(neural_net, X, Y, l2_cost, lr = 0.5, train= True): #lr Learning rate

  out= [(None,X)]# (valor de suma poderada, valor de activacion) (z,a)
  # forward pass
  for l, layer in enumerate(neural_net):

    z = out[-1][1] @ neural_net[l].W + neural_net[l].b # @ para multipilcar matricialmente
    a = neural_net[l].act_f[0](z)

    out.append((z,a))
##  print(l2_cost[0](out[-1][1], Y))
  
  if train:
    #Backward pass 
    deltas =[]
    for l in reversed(range(0, len(neural_net))):
      z= out[l+1][0]
      a= out[l+1][1]

      if l ==len(neural_net)-1:
        #calcular deta ultima capa
        deltas.insert(0,l2_cost[1](a,Y) * neural_net[l].act_f[1](a))
      else:
        #calcular delta respecto a capa previa 
        deltas.insert(0,deltas[0] @ _W.T * neural_net[l].act_f[1](a))

      _W = neural_net[l].W 

      #Gradient decent 
      neural_net[l].b = neural_net[l].b - np.mean(deltas[0],axis=0, keepdims=True) * lr
      neural_net[l].W = neural_net[l].W - out[l][1].T @ deltas[0] * lr

  return out[-1][1]

#train(neural_net, X, Y, l2_cost, 0.5)

import time 
from IPython.display import clear_output
neural_n = create_nn(topology, sigm)

loss = []
 
for i in range(2500):
  #entrenar la red
  pY = train (neural_n, X, Y, l2_cost, lr=0.05)
  
  if i% 25 ==0:
    print(pY)

    loss.append(l2_cost[0](pY, Y))

    resolucion = 50
    _x0 = np.linspace(-1.5, 1.5, resolucion)
    _x1 = np.linspace(-1.5, 1.5, resolucion)

    _Y = np.zeros((resolucion,resolucion))

    for i0, x0 in enumerate(_x0):
      for i1, x1 in enumerate(_x1):
        _Y[i0,i1] = train (neural_n, np.array([[x0,x1]]), Y, l2_cost, train=False)[0][0]

    plt.pcolormesh(_x0, _x1, _Y, cmap="coolwarm")
    plt.axis("equal")

    plt.scatter(X[Y[:,0] == 0, 0], X[Y[:,0] == 0, 1], c="skyblue")
    plt.scatter(X[Y[:,0] == 1, 0], X[Y[:,0] == 1, 1], c="salmon")

    clear_output(wait=True)
    plt.show()
    plt.plot(range(len(loss)), loss)
    plt.show()
    time.sleep(0.5)